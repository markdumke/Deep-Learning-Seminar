# Deep-Learning-Seminar
## Sequence Modeling: Recurrent and Recursive nets

Interesting links to Recurrent Neural Networks blog posts and papers:


Step-by-step explanation of LSTM networks
 - http://colah.github.io/posts/2015-08-Understanding-LSTMs/

Blog post on applications of RNNs
 - http://karpathy.github.io/2015/05/21/rnn-effectiveness/


Attention and other extensions of the basic RNN architecture

 - http://distill.pub/2016/augmented-rnns/


Blog post on attention and memory in Deep Learning

 - http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/


Implementation of RNN and LSTM in Python using Numpy and Theano
Part 1 - 4

 - http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/


Presentation on Recurrent Neural Networks, the Vanishing Gradient Problem and LSTM/GRU

 - https://www.nervanasys.com/recurrent-neural-networks/


Python implementation and tutorial on an RNN for sentiment analysis

 - http://deeplearning.net/tutorial/lstm.html#lstm


Python character level language model with mxnet

 - http://mxnet.io/tutorials/python/char_lstm.html


Generating text with an LSTM in R

 - https://www.r-bloggers.com/new-chapters-for-50-shades-of-grey/


Googles Machine Translation using encoder-decoder deep LSTM networks

 - https://arxiv.org/pdf/1611.04558v1.pdf
 - https://arxiv.org/pdf/1609.08144v2.pdf


Effective Approaches to Attention-based Neural Machine Translation
 - https://arxiv.org/pdf/1508.04025v5.pdf
